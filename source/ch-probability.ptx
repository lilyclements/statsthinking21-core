<?xml version="1.0" encoding="UTF-8" ?>
<chapter xml:id="ch-probability" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Probability</title>

  <introduction>
    <p>
      Probability theory is the branch of mathematics that deals with chance and uncertainty. It forms an important part of the foundation for statistics, because it provides us with the mathematical tools to describe uncertain events. The study of probability arose in part due to interest in understanding games of chance, like cards or dice. These games provide useful examples of many statistical concepts, because when we repeat these games the likelihood of different outcomes remains (mostly) the same. However, there are deep questions about the meaning of probability that we will not address here; see Suggested Readings at the end if you are interested in learning more about this fascinating topic and its history.
    </p>
  </introduction>

  <section xml:id="sec-probability-what-is-probability">
    <title>What is probability?</title>
    <p>
      Informally, we usually think of probability as a number that describes the likelihood of some event occurring, which ranges from zero (impossibility) to one (certainty). Sometimes probabilities will instead be expressed in percentages, which range from zero to one hundred, as when the weather forecast predicts a twenty percent chance of rain today. In each case, these numbers are expressing how likely that particular event is, ranging from absolutely impossible to absolutely certain.
    </p>
    <p>
      To formalize probability theory, we first need to define a few terms:
    </p>
    <ul>
      <li>An <em>experiment</em> is any activity that produces or observes an outcome. Examples are flipping a coin, rolling a 6-sided die, or trying a new route to work to see if it's faster than the old route.</li>
      <li>The <em>sample space</em> is the set of possible outcomes for an experiment. We represent these by listing them within a set of squiggly brackets. For a coin flip, the sample space is {heads, tails}. For a six-sided die, the sample space is each of the possible numbers that can appear: {1,2,3,4,5,6}. For the amount of time it takes to get to work, the sample space is all possible real numbers greater than zero (since it can't take a negative amount of time to get somewhere, at least not yet). We won't bother trying to write out all of those numbers within the brackets.</li>
      <li>An <em>event</em> is a subset of the sample space. In principle it could be one or more of possible outcomes in the sample space, but here we will focus primarily on <em>elementary events</em> which consist of exactly one possible outcome. For example, this could be obtaining heads in a single coin flip, rolling a 4 on a throw of the die, or taking 21 minutes to get home by the new route.</li>
    </ul>
    <p>
      Now that we have those definitions, we can outline the formal features of a probability, which were first defined by the Russian mathematician Andrei Kolmogorov. These are the features that a value <em>has</em> to have if it is going to be a probability. Let's say that we have a sample space defined by N independent events, <m>\{E_1, E_2, \ldots, E_N\}</m>, and <m>X</m> is a random variable denoting which of the events has occurred. <m>P(X=E_i)</m> is the probability of event <m>i</m>:
    </p>
    <ul>
      <li>Probability cannot be negative: <m>P(X=E_i) \ge 0</m></li>
      <li>The total probability of all outcomes in the sample space is 1; that is, if we take the probability of each <m>E_i</m> and add them up, they must sum to 1. We can express this using the summation symbol <m>\sum</m>:
      <me>
        \sum_{i=1}^N P(X=E_i) = P(X=E_1) + P(X=E_2) + \cdots + P(X=E_N) = 1
      </me>
      This is interpreted as saying <q>Take all of the N elementary events, which we have labeled from 1 to N, and add up their probabilities. These must sum to one.</q></li>
      <li>The probability of any individual event cannot be greater than one: <m>P(X=E_i) \le 1</m>. This is implied by the previous point; since they must sum to one, and they can't be negative, then any particular probability cannot exceed one.</li>
    </ul>
  </section>

  <section xml:id="sec-probability-determining-probabilities">
    <title>How do we determine probabilities?</title>
    <p>
      Now that we know what a probability is, how do we actually figure out what the probability is for any particular event?
    </p>

    <subsection xml:id="subsec-probability-personal-belief">
      <title>Personal belief</title>
      <p>
        Let's say that I asked you what the probability was that Bernie Sanders would have won the 2016 presidential election if he had been the democratic nominee instead of Hilary Clinton? We can't actually do the experiment to find the outcome. However, most people with knowledge of American politics would be willing to at least offer a guess at the probability of this event. In many cases personal knowledge and/or opinion is the only guide we have determining the probability of an event, but this is not very scientifically satisfying.
      </p>
    </subsection>

    <subsection xml:id="subsec-probability-empirical-frequency">
      <title>Empirical frequency</title>
      <p>
        Another way to determine the probability of an event is to do the experiment many times and count how often each event happens. From the relative frequency of the different outcomes, we can compute the probability of each outcome. For example, let's say that we are interested in knowing the probability of rain in San Francisco. We first have to define the experiment <mdash/> let's say that we will look at the National Weather Service data for each day in 2017 and determine whether there was any rain at the downtown San Francisco weather station. According to these data, in 2017 there were 73 rainy days. To compute the probability of rain in San Francisco, we simply divide the number of rainy days by the number of days counted (365), giving <m>P(\text{rain in SF in 2017}) = 0.2</m>.
      </p>
      <p>
        How do we know that empirical probability gives us the right number? The answer to this question comes from the <em>law of large numbers</em>, which shows that the empirical probability will approach the true probability as the sample size increases. We can see this by simulating a large number of coin flips, and looking at our estimate of the probability of heads after each flip. We will spend more time discussing simulation in a later chapter; for now, just assume that we have a computational way to generate a random outcome for each coin flip.
      </p>

      <program language="r">
        <input>
# Simulate coin flips to demonstrate law of large numbers
set.seed(12345)
nsamples &lt;- 30000  # number of flips

# Create random coin flips with true probability of 0.5
sampDf &lt;- 
  tibble( 
    trial_number = seq(nsamples), 
    outcomes = rbinom(nsamples, 1, 0.5)
  ) %&gt;%
  mutate(mean_probability = cumsum(outcomes) / seq_along(outcomes))

# Plot estimated probability vs trial number
sampDf %&gt;% 
  slice(10:nsamples) %&gt;%
  ggplot(aes(x = trial_number, y = mean_probability)) +
  geom_line() +
  geom_hline(yintercept = 0.5, color = "red")
        </input>
      </program>

      <p>
        A demonstration of the law of large numbers would show that as the number of coin flip trials increases, the estimated probability of heads converges onto the true value of 0.5. However, note that the estimates can be very far off from the true value when the sample sizes are small. A real-world example of this was seen in the 2017 special election for the US Senate in Alabama, which pitted the Republican Roy Moore against Democrat Doug Jones. Early in the evening the vote counts were especially volatile, swinging from a large initial lead for Jones to a long period where Moore had the lead, until finally Jones took the lead to win the race.
      </p>
      <p>
        These two examples show that while large samples will ultimately converge on the true probability, the results with small samples can be far off. Unfortunately, many people forget this and overinterpret results from small samples. This was referred to as the <em>law of small numbers</em> by the psychologists Danny Kahneman and Amos Tversky, who showed that people (even trained researchers) often behave as if the law of large numbers applies even to small samples, giving too much credence to results based on small datasets. We will see examples throughout the course of just how unstable statistical results can be when they are generated on the basis of small samples.
      </p>
    </subsection>

    <subsection xml:id="subsec-probability-classical-probability">
      <title>Classical probability</title>
      <p>
        It's unlikely that any of us has ever flipped a coin tens of thousands of times, but we are nonetheless willing to believe that the probability of flipping heads is 0.5. This reflects the use of yet another approach to computing probabilities, which we refer to as <em>classical probability</em>. In this approach, we compute the probability directly based on our knowledge of the situation.
      </p>
      <p>
        Classical probability arose from the study of games of chance such as dice and cards. A famous example arose from a problem encountered by a French gambler who went by the name of Chevalier de Méré. de Méré played two different dice games: In the first he bet on the chance of at least one six on four rolls of a six-sided die, while in the second he bet on the chance of at least one double-six on 24 rolls of two dice. He expected to win money on both of these gambles, but he found that while on average he won money on the first gamble, he actually lost money on average when he played the second gamble many times. To understand this he turned to his friend, the mathematician Blaise Pascal, who is now recognized as one of the founders of probability theory.
      </p>
      <p>
        How can we understand this question using probability theory? In classical probability, we start with the assumption that all of the elementary events in the sample space are equally likely; that is, when you roll a die, each of the possible outcomes ({1,2,3,4,5,6}) is equally likely to occur. (No loaded dice allowed!) Given this, we can compute the probability of any individual outcome as one divided by the number of possible outcomes:
      </p>
      <me>
        P(\text{outcome}_i) = \frac{1}{\text{number of possible outcomes}}
      </me>
      <p>
        For the six-sided die, the probability of each individual outcome is 1/6.
      </p>
      <p>
        This is nice, but de Méré was interested in more complex events, like what happens on multiple dice throws. How do we compute the probability of a complex event (which is a <em>union</em> of single events), like rolling a six on the first <em>or</em> the second throw? We represent the union of events mathematically using the <m>\cup</m> symbol: for example, if the probability of rolling a six on the first throw is referred to as <m>P(\text{Roll6}_{\text{throw1}})</m> and the probability of rolling a six on the second throw is <m>P(\text{Roll6}_{\text{throw2}})</m>, then the union is referred to as <m>P(\text{Roll6}_{\text{throw1}} \cup \text{Roll6}_{\text{throw2}})</m>.
      </p>
      <p>
        de Méré thought (incorrectly, as we will see below) that he could simply add together the probabilities of the individual events to compute the probability of the combined event, meaning that the probability of rolling a six on the first or second roll would be computed as follows:
      </p>
      <md>
        <mrow>P(\text{Roll6}_{\text{throw1}}) \amp = 1/6</mrow>
        <mrow>P(\text{Roll6}_{\text{throw2}}) \amp = 1/6</mrow>
      </md>
      <p>
        de Méré's error:
      </p>
      <me>
        P(\text{Roll6}_{\text{throw1}} \cup \text{Roll6}_{\text{throw2}}) = P(\text{Roll6}_{\text{throw1}}) + P(\text{Roll6}_{\text{throw2}}) = 1/6 + 1/6 = 1/3
      </me>
      <p>
        de Méré reasoned based on this incorrect assumption that the probability of at least one six in four rolls was the sum of the probabilities on each of the individual throws: <m>4 \cdot \frac{1}{6} = \frac{2}{3}</m>. Similarly, he reasoned that since the probability of a double-six when throwing two dice is 1/36, then the probability of at least one double-six on 24 rolls of two dice would be <m>24 \cdot \frac{1}{36} = \frac{2}{3}</m>. Yet, while he consistently won money on the first bet, he lost money on the second bet. What gives?
      </p>
      <p>
        To understand de Méré's error, we need to introduce some of the rules of probability theory. The first is the <em>rule of subtraction</em>, which says that the probability of some event A <em>not</em> happening is one minus the probability of the event happening:
      </p>
      <me>
        P(\neg A) = 1 - P(A)
      </me>
      <p>
        where <m>\neg A</m> means <q>not A</q>. This rule derives directly from the axioms that we discussed above; because A and <m>\neg A</m> are the only possible outcomes, then their total probability must sum to 1. For example, if the probability of rolling a one in a single throw is <m>\frac{1}{6}</m>, then the probability of rolling anything other than a one is <m>\frac{5}{6}</m>.
      </p>
      <p>
        A second rule tells us how to compute the probability of a conjoint event <mdash/> that is, the probability that both of two events will occur. We refer to this as an <em>intersection</em>, which is signified by the <m>\cap</m> symbol; thus, <m>P(A \cap B)</m> means the probability that both A and B will occur. We will focus on a version of the rule that tells us how to compute this quantity in the special case when the two events are independent from one another; we will learn later exactly what the concept of <em>independence</em> means, but for now we can just take it for granted that the two die throws are independent events. We compute the probability of the intersection of two independent events by simply multiplying the probabilities of the individual events:
      </p>
      <me>
        P(A \cap B) = P(A) \cdot P(B) \text{ if and only if A and B are independent}
      </me>
      <p>
        Thus, the probability of throwing a six on both of two rolls is <m>\frac{1}{6} \cdot \frac{1}{6} = \frac{1}{36}</m>.
      </p>
      <p>
        The third rule tells us how to add together probabilities <mdash/> and it is here that we see the source of de Méré's error. The addition rule tells us that to obtain the probability of either of two events occurring, we add together the individual probabilities, but then subtract the likelihood of both occurring together:
      </p>
      <me>
        P(A \cup B) = P(A) + P(B) - P(A \cap B)
      </me>
      <p>
        In a sense, this prevents us from counting those instances twice, and that's what distinguishes the rule from de Méré's incorrect computation. Let's say that we want to find the probability of rolling 6 on either of two throws. According to our rules:
      </p>
      <md>
        <mrow>P(\text{Roll6}_{\text{throw1}} \cup \text{Roll6}_{\text{throw2}}) \amp = P(\text{Roll6}_{\text{throw1}}) + P(\text{Roll6}_{\text{throw2}}) - P(\text{Roll6}_{\text{throw1}} \cap \text{Roll6}_{\text{throw2}})</mrow>
        <mrow>\amp = \frac{1}{6} + \frac{1}{6} - \frac{1}{36} = \frac{11}{36}</mrow>
      </md>
      <p>
        A matrix representation would show each cell representing one outcome of two throws of a die, with the columns representing the first throw and the rows representing the second throw. Cells with a six in either the first or second throw would be highlighted. If you count up the highlighted cells you will see that there are 11 such cells. This shows why the addition rule gives a different answer from de Méré's; if we were to simply add together the probabilities for the two throws as he did, then we would count (6,6) towards both, when it should really only be counted once.
      </p>
    </subsection>

    <subsection xml:id="subsec-probability-solving-demere">
      <title>Solving de Méré's problem</title>
      <p>
        Blaise Pascal used the rules of probability to come up with a solution to de Méré's problem. First, he realized that computing the probability of at least one event out of a combination was tricky, whereas computing the probability that something does not occur across several events is relatively easy <mdash/> it's just the product of the probabilities of the individual events. Thus, rather than computing the probability of at least one six in four rolls, he instead computed the probability of no sixes across all rolls:
      </p>
      <me>
        P(\text{no sixes in four rolls}) = \frac{5}{6} \cdot \frac{5}{6} \cdot \frac{5}{6} \cdot \frac{5}{6} = \left(\frac{5}{6}\right)^4 = 0.482
      </me>
      <p>
        He then used the fact that the probability of no sixes in four rolls is the complement of at least one six in four rolls (thus they must sum to one), and used the rule of subtraction to compute the probability of interest:
      </p>
      <me>
        P(\text{at least one six in four rolls}) = 1 - \left(\frac{5}{6}\right)^4 = 0.517
      </me>
      <p>
        de Méré's gamble that he would throw at least one six in four rolls has a probability of greater than 0.5, explaining why de Méré made money on this bet on average.
      </p>
      <p>
        But what about de Méré's second bet? Pascal used the same trick:
      </p>
      <md>
        <mrow>P(\text{no double six in 24 rolls}) \amp = \left(\frac{35}{36}\right)^{24} = 0.509</mrow>
        <mrow>P(\text{at least one double six in 24 rolls}) \amp = 1 - \left(\frac{35}{36}\right)^{24} = 0.491</mrow>
      </md>
      <p>
        The probability of this outcome was slightly below 0.5, showing why de Méré lost money on average on this bet.
      </p>
    </subsection>
  </section>

  <section xml:id="sec-probability-distributions">
    <title>Probability distributions</title>
    <p>
      A <em>probability distribution</em> describes the probability of all of the possible outcomes in an experiment. For example, on Jan 20 2018, the basketball player Steph Curry hit only 2 out of 4 free throws in a game against the Houston Rockets. We know that Curry's overall probability of hitting free throws across the entire season was 0.91, so it seems pretty unlikely that he would hit only 50% of his free throws in a game, but exactly how unlikely is it? We can determine this using a theoretical probability distribution; throughout this book we will encounter a number of these probability distributions, each of which is appropriate to describe different types of data. In this case, we use the <em>binomial</em> distribution, which provides a way to compute the probability of some number of successes out of a number of trials on which there is either success or failure and nothing in between (known as <q>Bernoulli trials</q>), given some known probability of success on each trial. This distribution is defined as:
    </p>
    <me>
      P(k; n,p) = P(X=k) = \binom{n}{k} p^k(1-p)^{n-k}
    </me>
    <p>
      This refers to the probability of k successes on n trials when the probability of success is p. You may not be familiar with <m>\binom{n}{k}</m>, which is referred to as the <em>binomial coefficient</em>. The binomial coefficient is also referred to as <q>n-choose-k</q> because it describes the number of different ways that one can choose k items out of n total items. The binomial coefficient is computed as:
    </p>
    <me>
      \binom{n}{k} = \frac{n!}{k!(n-k)!}
    </me>
    <p>
      where the exclamation point (!) refers to the <em>factorial</em> of the number:
    </p>
    <me>
      n! = \prod_{i=1}^n i = n \cdot (n-1) \cdot \ldots \cdot 2 \cdot 1
    </me>
    <p>
      The product operator <m>\prod</m> is similar to the summation operator <m>\sum</m>, except that it multiplies instead of adds. In this case, it is multiplying together all numbers from one to <m>n</m>.
    </p>
    <p>
      In the example of Steph Curry's free throws:
    </p>
    <me>
      P(2; 4, 0.91) = \binom{4}{2} 0.91^2(1-0.91)^{4-2} = 0.040
    </me>
    <p>
      This shows that given Curry's overall free throw percentage, it is very unlikely that he would hit only 2 out of 4 free throws. Which just goes to show that unlikely things do actually happen in the real world.
    </p>

    <subsection xml:id="subsec-probability-cumulative-distributions">
      <title>Cumulative probability distributions</title>
      <p>
        Often we want to know not just how likely a specific value is, but how likely it is to find a value that is as extreme or more than a particular value; this will become very important when we discuss hypothesis testing in Chapter 9. To answer this question, we can use a <em>cumulative</em> probability distribution; whereas a standard probability distribution tells us the probability of some specific value, the cumulative distribution tells us the probability of a value as large or larger (or as small or smaller) than some specific value.
      </p>
      <p>
        In the free throw example, we might want to know: What is the probability that Steph Curry hits 2 <em>or fewer</em> free throws out of four, given his overall free throw probability of 0.91. To determine this, we could simply use the binomial probability equation and plug in all of the possible values of k and add them together:
      </p>
      <me>
        P(k \le 2) = P(k=2) + P(k=1) + P(k=0) = 6 \times 10^{-5} + 0.002 + 0.040 = 0.043
      </me>
      <p>
        In many cases the number of possible outcomes would be too large for us to compute the cumulative probability by enumerating all possible values; fortunately, it can be computed directly for any theoretical probability distribution. A table showing the cumulative probability of each possible number of successful free throws would show that the probability of Curry landing 2 or fewer free throws out of 4 attempts is 0.043.
      </p>
    </subsection>
  </section>

  <section xml:id="sec-probability-conditional-probability">
    <title>Conditional probability</title>
    <p>
      So far we have limited ourselves to simple probabilities <mdash/> that is, the probability of a single event or combination of events. However, we often wish to determine the probability of some event given that some other event has occurred, which are known as <em>conditional probabilities</em>.
    </p>
    <p>
      Let's take the 2016 US Presidential election as an example. There are two simple probabilities that we could use to describe the electorate. First, we know the probability that a voter in the US is affiliated with the Republican party: <m>P(\text{Republican}) = 0.44</m>. We also know the probability that a voter cast their vote in favor of Donald Trump: <m>P(\text{Trump voter}) = 0.46</m>. However, let's say that we want to know the following: What is the probability that a person cast their vote for Donald Trump, <em>given that they are a Republican</em>?
    </p>
    <p>
      To compute the conditional probability of A given B (which we write as <m>P(A|B)</m>, <q>probability of A, given B</q>), we need to know the <em>joint probability</em> (that is, the probability of both A and B occurring) as well as the overall probability of B:
    </p>
    <me>
      P(A|B) = \frac{P(A \cap B)}{P(B)}
    </me>
    <p>
      That is, we want to know the probability that both things are true, given that the one being conditioned upon is true.
    </p>
    <p>
      It can be useful to think of this graphically. A flow chart would show how the full population of voters breaks down into Republicans and Democrats, and how the conditional probability (conditioning on party) further breaks down the members of each party according to their vote.
    </p>
  </section>

  <section xml:id="sec-probability-computing-conditional">
    <title>Computing conditional probabilities from data</title>
    <p>
      We can also compute conditional probabilities directly from data. Let's say that we are interested in the following question: What is the probability that someone has diabetes, given that they are not physically active? <mdash/> that is, <m>P(\text{diabetes}|\text{inactive})</m>. The NHANES dataset includes two variables that address the two parts of this question. The first (Diabetes) asks whether the person has ever been told that they have diabetes, and the second (PhysActive) records whether the person engages in sports, fitness, or recreational activities that are at least of moderate intensity. We can first compute the simple probabilities. The data would show that the probability that someone in the NHANES dataset has diabetes is 0.1, and the probability that someone is inactive is 0.45.
    </p>
    <p>
      To compute <m>P(\text{diabetes}|\text{inactive})</m> we would also need to know the joint probability of being diabetic <em>and</em> inactive, in addition to the simple probabilities of each. Based on these joint probabilities, we can compute <m>P(\text{diabetes}|\text{inactive})</m>. One way to do this in a computer program is to first determine whether the PhysActive variable was equal to <q>No</q> for each individual, and then take the mean of those truth values. Since TRUE/FALSE values are treated as 1/0 respectively by most programming languages (including R and Python), this allows us to easily identify the probability of a simple event by simply taking the mean of a logical variable representing its truth value. We then use that value to compute the conditional probability, where we might find that the probability of someone having diabetes given that they are physically inactive is approximately 0.14.
    </p>
  </section>

  <section xml:id="sec-probability-independence">
    <title>Independence</title>
    <p>
      The term <q>independent</q> has a very specific meaning in statistics, which is somewhat different from the common usage of the term. Statistical independence between two variables means that knowing the value of one variable doesn't tell us anything about the value of the other. This can be expressed as:
    </p>
    <me>
      P(A|B) = P(A)
    </me>
    <p>
      That is, the probability of A given some value of B is just the same as the overall probability of A. Looking at it this way, we see that many cases of what we would call <q>independence</q> in the real world are not actually statistically independent. For example, there is currently a move by a small group of California citizens to declare a new independent state called Jefferson, which would comprise a number of counties in northern California and Oregon. If this were to happen, then the probability that a current California resident would now live in the state of Jefferson would be <m>P(\text{Jeffersonian}) = 0.014</m>, whereas the probability that they would remain a California resident would be <m>P(\text{Californian}) = 0.986</m>. The new states might be politically independent, but they would <em>not</em> be statistically independent, because if we know that a person is Jeffersonian, then we can be sure that they are <em>not</em> Californian! That is, while independence in common language often refers to sets that are exclusive, statistical independence refers to the case where one cannot predict anything about one variable from the value of another variable. For example, knowing a person's hair color is unlikely to tell you whether they prefer chocolate or strawberry ice cream.
    </p>
    <p>
      Let's look at another example, using the NHANES data: Are physical health and mental health independent of one another? NHANES includes two relevant questions: PhysActive, which asks whether the individual is physically active, and DaysMentHlthBad, which asks how many days out of the last 30 that the individual experienced bad mental health. Let's consider anyone who had more than 7 days of bad mental health in the last month to be in bad mental health. Based on this, we can define a new variable called badMentalHealth as a logical variable telling whether each person had more than 7 days of bad mental health or not. We can first summarize the data to show how many individuals fall into each combination of the two variables, and then divide by the total number of observations to create a table of proportions.
    </p>
    <p>
      This shows us the proportion of all observations that fall into each cell. However, what we want to know here is the conditional probability of bad mental health, depending on whether one is physically active or not. To compute this, we divide each physical activity group by its total number of observations, so that each row now sums to one. Here we see the conditional probabilities of bad or good mental health for each physical activity group (in the top two rows) along with the overall probability of good or bad mental health in the third row. To determine whether mental health and physical activity are independent, we would compare the simple probability of bad mental health (in the third row) to the conditional probability of bad mental health given that one is physically active (in the second row).
    </p>
    <p>
      The overall probability of bad mental health <m>P(\text{bad mental health})</m> might be, for example, 0.13, while the conditional probability <m>P(\text{bad mental health}|\text{physically active})</m> might be 0.11. Thus, it seems that the conditional probability is somewhat smaller than the overall probability, suggesting that they are not independent, though we can't know for sure just by looking at the numbers, since these numbers might be different due to random variability in our sample. Later in the book we will discuss statistical tools that will let us directly test whether two variables are independent.
    </p>
  </section>

  <section xml:id="sec-probability-bayes-rule">
    <title>Reversing a conditional probability: Bayes' rule</title>
    <p>
      In many cases, we know <m>P(A|B)</m> but we really want to know <m>P(B|A)</m>. This commonly occurs in medical screening, where we know <m>P(\text{positive test result}|\text{disease})</m> but what we want to know is <m>P(\text{disease}|\text{positive test result})</m>. For example, some doctors recommend that men over the age of 50 undergo screening using a test called prostate specific antigen (PSA) to screen for possible prostate cancer. Before a test is approved for use in medical practice, the manufacturer needs to test two aspects of the test's performance. First, they need to show how <em>sensitive</em> it is <mdash/> that is, how likely is it to find the disease when it is present: <m>\text{sensitivity} = P(\text{positive test}|\text{disease})</m>. They also need to show how <em>specific</em> it is: that is, how likely is it to give a negative result when there is no disease present: <m>\text{specificity} = P(\text{negative test}|\text{no disease})</m>. For the PSA test, we know that sensitivity is about 80% and specificity is about 70%. However, these don't answer the question that the physician wants to answer for any particular patient: what is the likelihood that they actually have cancer, given that the test comes back positive? This requires that we reverse the conditional probability that defines sensitivity: instead of <m>P(\text{positive test}|\text{disease})</m> we want to know <m>P(\text{disease}|\text{positive test})</m>.
    </p>
    <p>
      In order to reverse a conditional probability, we can use <em>Bayes' rule</em>:
    </p>
    <me>
      P(B|A) = \frac{P(A|B) \cdot P(B)}{P(A)}
    </me>
    <p>
      Bayes' rule is fairly easy to derive, based on the rules of probability that we learned earlier in the chapter (see the Appendix for this derivation).
    </p>
    <p>
      If we have only two outcomes, we can express Bayes' rule in a somewhat clearer way, using the sum rule to redefine <m>P(A)</m>:
    </p>
    <me>
      P(A) = P(A|B) \cdot P(B) + P(A|\neg B) \cdot P(\neg B)
    </me>
    <p>
      Using this, we can redefine Bayes's rule:
    </p>
    <me>
      P(B|A) = \frac{P(A|B) \cdot P(B)}{P(A|B) \cdot P(B) + P(A|\neg B) \cdot P(\neg B)}
    </me>
    <p>
      We can plug the relevant numbers into this equation to determine the likelihood that an individual with a positive PSA result actually has cancer <mdash/> but note that in order to do this, we also need to know the overall probability of cancer for that person, which we often refer to as the <em>base rate</em>. Let's take a 60 year old man, for whom the probability of prostate cancer in the next 10 years is <m>P(\text{cancer}) = 0.058</m>. Using the sensitivity and specificity values that we outlined above, we can compute the individual's likelihood of having cancer given a positive test:
    </p>
    <md>
      <mrow>P(\text{cancer}|\text{test}) \amp = \frac{P(\text{test}|\text{cancer}) \cdot P(\text{cancer})}{P(\text{test}|\text{cancer}) \cdot P(\text{cancer}) + P(\text{test}|\neg\text{cancer}) \cdot P(\neg\text{cancer})}</mrow>
      <mrow>\amp = \frac{0.8 \cdot 0.058}{0.8 \cdot 0.058 + 0.3 \cdot 0.942} = 0.14</mrow>
    </md>
    <p>
      That's pretty small <mdash/> do you find that surprising? Many people do, and in fact there is a substantial psychological literature showing that people systematically neglect <em>base rates</em> (i.e. overall prevalence) in their judgments.
    </p>
  </section>

  <section xml:id="sec-probability-learning-from-data">
    <title>Learning from data</title>
    <p>
      Another way to think of Bayes' rule is as a way to update our beliefs on the basis of data <mdash/> that is, learning about the world using data. Let's look at Bayes' rule again:
    </p>
    <me>
      P(B|A) = \frac{P(A|B) \cdot P(B)}{P(A)}
    </me>
    <p>
      The different parts of Bayes' rule have specific names, that relate to their role in using Bayes' rule to update our beliefs. We start out with an initial guess about the probability of B (<m>P(B)</m>), which we refer to as the <em>prior</em> probability. In the PSA example we used the base rate as our prior, since it was our best guess as to the individual's chance of cancer before we knew the test result. We then collect some data, which in our example was the test result. The degree to which the data A are consistent with outcome B is given by <m>P(A|B)</m>, which we refer to as the <em>likelihood</em>. You can think of this as how likely the data are, given that the particular hypothesis being tested is true. In our example, the hypothesis being tested was whether the individual had cancer, and the likelihood was based on our knowledge about the sensitivity of the test (that is, the probability of a positive test outcome given cancer is present). The denominator (<m>P(A)</m>) is referred to as the <em>marginal likelihood</em>, because it expresses the overall likelihood of the data, averaged across all of the possible values of B (which in our example were disease present and disease absent). The outcome to the left (<m>P(B|A)</m>) is referred to as the <em>posterior</em> <mdash/> because it's what comes out the back end of the computation.
    </p>
    <p>
      There is another way of writing Bayes rule that makes this a bit clearer:
    </p>
    <me>
      P(B|A) = \frac{P(A|B)}{P(A)} \cdot P(B)
    </me>
    <p>
      The part on the left (<m>\frac{P(A|B)}{P(A)}</m>) tells us how much more or less likely the data A are given B, relative to the overall (marginal) likelihood of the data, while the part on the right side (<m>P(B)</m>) tells us how likely we thought B was before we knew anything about the data. This makes it clearer that the role of Bayes theorem is to update our prior knowledge based on the degree to which the data are more likely given B than they would be overall. If the hypothesis is more likely given the data than it would be in general, then we increase our belief in the hypothesis; if it's less likely given the data, then we decrease our belief.
    </p>
  </section>

  <section xml:id="sec-probability-odds-ratios">
    <title>Odds and odds ratios</title>
    <p>
      The result in the last section showed that the likelihood that the individual has cancer based on a positive PSA test result is still fairly low, even though it's more than twice as big as it was before we knew the test result. We would often like to quantify the relation between probabilities more directly, which we can do by converting them into <em>odds</em> which express the relative likelihood of something happening or not:
    </p>
    <me>
      \text{odds of A} = \frac{P(A)}{P(\neg A)}
    </me>
    <p>
      In our PSA example, the odds of having cancer (given the positive test) are:
    </p>
    <me>
      \text{odds of cancer} = \frac{P(\text{cancer})}{P(\neg \text{cancer})} = \frac{0.14}{1 - 0.14} = 0.16
    </me>
    <p>
      This tells us that the odds are fairly low of having cancer, even though the test was positive. For comparison, the odds of rolling a 6 in a single dice throw are:
    </p>
    <me>
      \text{odds of 6} = \frac{1}{5} = 0.2
    </me>
    <p>
      As an aside, this is a reason why many medical researchers have become increasingly wary of the use of widespread screening tests for relatively uncommon conditions; most positive results will turn out to be false positives, resulting in unnecessary followup tests with possible complications, not to mention added stress for the patient.
    </p>
    <p>
      We can also use odds to compare different probabilities, by computing what is called an <em>odds ratio</em> <mdash/> which is exactly what it sounds like. For example, let's say that we want to know how much the positive test increases the individual's odds of having cancer. We can first compute the <em>prior odds</em> <mdash/> that is, the odds before we knew that the person had tested positively. These are computed using the base rate:
    </p>
    <me>
      \text{prior odds} = \frac{P(\text{cancer})}{P(\neg \text{cancer})} = \frac{0.058}{1 - 0.058} = 0.061
    </me>
    <p>
      We can then compare these with the posterior odds, which are computed using the posterior probability:
    </p>
    <me>
      \text{odds ratio} = \frac{\text{posterior odds}}{\text{prior odds}} = \frac{0.16}{0.061} = 2.62
    </me>
    <p>
      This tells us that the odds of having cancer are increased by 2.62 times given the positive test result. An odds ratio is an example of what we will later call an <em>effect size</em>, which is a way of quantifying how relatively large any particular statistical effect is.
    </p>
  </section>

  <section xml:id="sec-probability-meaning">
    <title>What do probabilities mean?</title>
    <p>
      It might strike you that it is a bit odd to talk about the probability of a person having cancer depending on a test result; after all, the person either has cancer or they don't. Historically, there have been two different ways that probabilities have been interpreted. The first (known as the <em>frequentist</em> interpretation) interprets probabilities in terms of long-run frequencies. For example, in the case of a coin flip, it would reflect the relative frequencies of heads in the long run after a large number of flips. While this interpretation might make sense for events that can be repeated many times like a coin flip, it makes less sense for events that will only happen once, like an individual person's life or a particular presidential election; and as the economist John Maynard Keynes famously said, <q>In the long run, we are all dead.</q>
    </p>
    <p>
      The other interpretation of probabilities (known as the <em>Bayesian</em> interpretation) is as a degree of belief in a particular proposition. If I were to ask you <q>How likely is it that the US will return to the moon by 2040</q>, you can provide an answer to this question based on your knowledge and beliefs, even though there are no relevant frequencies to compute a frequentist probability. One way that we often frame subjective probabilities is in terms of one's willingness to accept a particular gamble. For example, if you think that the probability of the US landing on the moon by 2040 is 0.1 (i.e. odds of 9 to 1), then that means that you should be willing to accept a gamble that would pay off with anything more than 9 to 1 odds if the event occurs.
    </p>
    <p>
      As we will see, these two different definitions of probability are very relevant to the two different ways that statisticians think about testing statistical hypotheses, which we will encounter in later chapters.
    </p>
  </section>

  <section xml:id="sec-probability-learning-objectives">
    <title>Learning objectives</title>
    <p>
      Having read this chapter, you should be able to:
    </p>
    <ul>
      <li>Describe the sample space for a selected random experiment.</li>
      <li>Compute relative frequency and empirical probability for a given set of events.</li>
      <li>Compute probabilities of single events, complementary events, and the unions and intersections of collections of events.</li>
      <li>Describe the law of large numbers.</li>
      <li>Describe the difference between a probability and a conditional probability.</li>
      <li>Describe the concept of statistical independence.</li>
      <li>Use Bayes' theorem to compute the inverse conditional probability.</li>
    </ul>
  </section>

  <section xml:id="sec-probability-suggested-readings">
    <title>Suggested readings</title>
    <ul>
      <li><em>The Drunkard's Walk: How Randomness Rules Our Lives</em>, by Leonard Mlodinow</li>
      <li><em>Ten Great Ideas about Chance</em>, by Persi Diaconis and Brian Skyrms</li>
    </ul>
  </section>

  <section xml:id="sec-probability-appendix">
    <title>Appendix</title>

    <subsection xml:id="subsec-probability-derivation-bayes">
      <title>Derivation of Bayes' rule</title>
      <p>
        First, remember the rule for computing a conditional probability:
      </p>
      <me>
        P(A|B) = \frac{P(A \cap B)}{P(B)}
      </me>
      <p>
        We can rearrange this to get the formula to compute the joint probability using the conditional:
      </p>
      <me>
        P(A \cap B) = P(A|B) \cdot P(B)
      </me>
      <p>
        Using this we can compute the inverse probability:
      </p>
      <me>
        P(B|A) = \frac{P(A \cap B)}{P(A)} = \frac{P(A|B) \cdot P(B)}{P(A)}
      </me>
    </subsection>
  </section>

</chapter>
