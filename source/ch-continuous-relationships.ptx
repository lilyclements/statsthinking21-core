<?xml version="1.0" encoding="UTF-8" ?>
<chapter xml:id="ch-continuous-relationships" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Modeling Continuous Relationships</title>

  <introduction>
    <p>
      Most people are familiar with the concept of <em>correlation</em>, and in this chapter we will provide a more formal understanding for this commonly used and misunderstood concept.

    <program language="r">
      <input>
      # Pearson correlation

    <program language="r">
      <input>
      # Pearson correlation coefficient
      cor.test(data$x, data$y, method = "pearson")

      # Correlation matrix for multiple variables
      cor(data[, c("var1", "var2", "var3")], use = "complete.obs")

      # Spearman's rank correlation (non-parametric)
      cor.test(data$x, data$y, method = "spearman")
      </input>
    </program>
      cor.test(NHANES_adult$Height, NHANES_adult$Weight)

      # Simple linear regression

    <program language="r">
      <input>
      # Simple linear regression
      model &lt;- lm(y ~ x, data = data)
      summary(model)

      # Extract coefficients
      coef(model)

      # Confidence intervals for coefficients
      confint(model)

      # Predictions
      new_data &lt;- data.frame(x = c(1, 2, 3))
      predict(model, new_data, interval = "confidence")

      # Add regression line to plot
      plot(data$x, data$y)
      abline(model, col = "red", lwd = 2)
      </input>
    </program>
      model &lt;- lm(Weight ~ Height, data = NHANES_adult)
      summary(model)
      </input>
    </program>
    </p>
  </introduction>

  <section xml:id="sec-hate-crimes-example">
    <title>An example: Hate crimes and income inequality</title>
    <p>
      In 2017, the web site Fivethirtyeight.com published a story titled <url href="https://fivethirtyeight.com/features/higher-rates-of-hate-crimes-are-tied-to-income-inequality/" visual="fivethirtyeight.com"><q>Higher Rates Of Hate Crimes Are Tied To Income Inequality</q></url> which discussed the relationship between the prevalence of hate crimes and income inequality in the wake of the 2016 Presidential election. The story reported an analysis of hate crime data from the FBI and the Southern Poverty Law Center, on the basis of which they report:
    </p>
    <blockquote>
      <p>
        we found that income inequality was the most significant determinant of population-adjusted hate crimes and hate incidents across the United States.
      </p>
    </blockquote>
    <p>
      The data for this analysis are available as part the <c>fivethirtyeight</c> package for the R statistical software, which makes it easy for us to access them. The analysis reported in the story focused on the relationship between income inequality (defined by a quantity called the <em>Gini index</em> <mdash/> see Appendix for more details) and the prevalence of hate crimes in each state.
    </p>
  </section>

  <section xml:id="sec-income-inequality-hate-crimes">
    <title>Is income inequality related to hate crimes?</title>

    <figure xml:id="fig-hate-crime-gini">
      <caption>Plot of rates of hate crimes vs. Gini index.</caption>
      <image source="hateCrimeGini.png" width="75%">
        <description>A scatter plot showing the relationship between Gini index (x-axis, ranging from 0.4 to 0.55) and average hate crimes per 100K population from FBI data (y-axis). Each point represents a U.S. state labeled with its two-letter abbreviation. The plot shows a generally positive trend.</description>
      </image>
    </figure>

    <p>
      The relationship between income inequality and rates of hate crimes is shown in <xref ref="fig-hate-crime-gini"/>. Looking at the data, it seems that there may be a positive relationship between the two variables. How can we quantify that relationship?
    </p>
  </section>

  <section xml:id="sec-covariance-correlation">
    <title>Covariance and correlation</title>
    <p>
      One way to quantify the relationship between two variables is the <em>covariance</em>. Remember that variance for a single variable is computed as the average squared difference between each data point and the mean:
    </p>
    <me>
      s^2 = \frac{\sum_{i=1}^n (x_i - \bar{x})^2}{N - 1}
    </me>
    <p>
      This tells us how far each observation is from the mean, on average, in squared units. Covariance tells us whether there is a relation between the deviations of two different variables across observations. It is defined as:
    </p>
    <me>
      \text{covariance} = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{N - 1}
    </me>
    <p>
      This value will be far from zero when individual data points deviate by similar amounts from their respective means; if they are deviant in the same direction then the covariance is positive, whereas if they are deviant in opposite directions the covariance is negative. Let's look at a toy example first. The data are shown in <xref ref="table-cov-example"/>, along with their individual deviations from the mean and their crossproducts.
    </p>

    <table xml:id="table-cov-example">
      <title>Data for toy example of covariance</title>
      <tabular halign="center">
        <row header="yes" bottom="minor">
          <cell>x</cell>
          <cell>y</cell>
          <cell>y_dev</cell>
          <cell>x_dev</cell>
          <cell>crossproduct</cell>
        </row>
        <row>
          <cell>3</cell>
          <cell>5</cell>
          <cell>-2.4</cell>
          <cell>-4.6</cell>
          <cell>11.04</cell>
        </row>
        <row>
          <cell>5</cell>
          <cell>3</cell>
          <cell>-4.4</cell>
          <cell>-2.6</cell>
          <cell>11.44</cell>
        </row>
        <row>
          <cell>8</cell>
          <cell>8</cell>
          <cell>0.6</cell>
          <cell>0.4</cell>
          <cell>0.24</cell>
        </row>
        <row>
          <cell>10</cell>
          <cell>9</cell>
          <cell>1.6</cell>
          <cell>2.4</cell>
          <cell>3.84</cell>
        </row>
        <row>
          <cell>12</cell>
          <cell>12</cell>
          <cell>4.6</cell>
          <cell>4.4</cell>
          <cell>20.24</cell>
        </row>
      </tabular>
    </table>

    <p>
      The covariance is simply the mean of the crossproducts, which in this case is 11.7. We don't usually use the covariance to describe relationships between variables, because it varies with the overall level of variance in the data. Instead, we would usually use the <em>correlation coefficient</em> (often referred to as <em>Pearson's correlation</em> after the statistician Karl Pearson). The correlation is computed by scaling the covariance by the standard deviations of the two variables:
    </p>
    <me>
      r = \frac{\text{covariance}}{s_xs_y} = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{(N - 1)s_x s_y}
    </me>
    <p>
      In this case, the value is 0.99. The correlation coefficient is useful because it varies between -1 and 1 regardless of the nature of the data <mdash/> in fact, we already discussed the correlation coefficient earlier in our discussion of effect sizes. As we saw in that previous chapter, a correlation of 1 indicates a perfect linear relationship, a correlation of -1 indicates a perfect negative relationship, and a correlation of zero indicates no linear relationship.
    </p>

    <subsection xml:id="subsec-hypothesis-testing-correlations">
      <title>Hypothesis testing for correlations</title>
      <p>
        The correlation value of 0.40 between hate crimes and income inequality seems to indicate a reasonably strong relationship between the two, but we can also imagine that this could occur by chance even if there is no relationship. We can test the null hypothesis that the correlation is zero, using a simple equation that lets us convert a correlation value into a <em>t</em> statistic:
      </p>
      <me>
        \textit{t}_r = \frac{r\sqrt{N-2}}{\sqrt{1-r^2}}
      </me>
      <p>
        Under the null hypothesis <m>H_0:r=0</m>, this statistic is distributed as a t distribution with <m>N - 2</m> degrees of freedom. We can compute this using our statistical software:
      </p>
      <p>
        For the hate crime data, the test gives:
      </p>
      <ul>
        <li><p>t = 3.0412</p></li>
        <li><p>df = 49</p></li>
        <li><p>p-value = 0.003844</p></li>
        <li><p>95 percent confidence interval: [0.1491051, 0.6283134]</p></li>
        <li><p>correlation: 0.3977146</p></li>
      </ul>
      <p>
        This test shows that the likelihood of an r value this extreme or more is quite low under the null hypothesis, so we would reject the null hypothesis of <m>r=0</m>. Note that this test assumes that both variables are normally distributed.
      </p>
      <p>
        We could also test this by randomization, in which we repeatedly shuffle the values of one of the variables and compute the correlation, and then compare our observed correlation value to this null distribution to determine how likely our observed value would be under the null hypothesis. The results are shown in <xref ref="fig-shuffle-corr"/>. The p-value computed using randomization is reasonably similar to the answer given by the t-test.
      </p>

      <figure xml:id="fig-shuffle-corr">
        <caption>Histogram of correlation values under the null hypothesis, obtained by shuffling values. Observed value is denoted by blue line.</caption>
        <image source="shuffleCorr.png" width="50%">
          <description>A histogram showing the distribution of correlation coefficients from shuffled data. The distribution is centered near zero and roughly bell-shaped. A blue vertical line marks the observed correlation value in the right tail of the distribution, with a p-value of approximately 0.004.</description>
        </image>
      </figure>

      <p>
        We could also use Bayesian inference to estimate the correlation; see the Appendix for more on this.
      </p>
    </subsection>

    <subsection xml:id="subsec-robust-correlations">
      <title>Robust correlations</title>
      <p>
        You may have noticed something a bit odd in <xref ref="fig-hate-crime-gini"/> <mdash/> one of the datapoints (the one for the District of Columbia) seemed to be quite separate from the others. We refer to this as an <em>outlier</em>, and the standard correlation coefficient is very sensitive to outliers. For example, in <xref ref="fig-outlier-corr"/> we can see how a single outlying data point can cause a very high positive correlation value, even when the actual relationship between the other data points is perfectly negative.
      </p>

      <figure xml:id="fig-outlier-corr">
        <caption>A simulated example of the effects of outliers on correlation. Without the outlier the remainder of the datapoints have a perfect negative correlation, but the single outlier changes the correlation value to highly positive.</caption>
        <image source="outlierCorr.png" width="50%">
          <description>A scatter plot showing ten points. Nine points form a perfect negative linear relationship, but one extreme outlier point in the upper right changes the overall correlation from -1.0 to 0.85.</description>
        </image>
      </figure>

      <p>
        One way to address outliers is to compute the correlation on the ranks of the data after ordering them, rather than on the data themselves; this is known as the <em>Spearman correlation</em>. Whereas the Pearson correlation for the example in <xref ref="fig-outlier-corr"/> was 0.85, the Spearman correlation is -0.91, showing that the rank correlation reduces the effect of the outlier and reflects the negative relationship between the majority of the data points.
      </p>
      <p>
        We can compute the rank correlation on the hate crime data as well. Using Spearman's method, we get:
      </p>
      <ul>
        <li><p>S = 26522</p></li>
        <li><p>p-value = 0.8384</p></li>
        <li><p>rho = 0.02933799</p></li>
      </ul>
      <p>
        Now we see that the correlation is no longer significant (and in fact is very near zero), suggesting that the claims of the FiveThirtyEight blog post may have been incorrect due to the effect of the outlier.
      </p>
    </subsection>
  </section>

  <section xml:id="sec-correlation-causation">
    <title>Correlation and causation</title>
    <p>
      When we say that one thing <em>causes</em> another, what do we mean? There is a long history in philosophy of discussion about the meaning of causality, but in statistics one way that we commonly think of causation is in terms of experimental control. That is, if we think that factor X causes factor Y, then manipulating the value of X should also change the value of Y.
    </p>
    <p>
      In medicine, there is a set of ideas known as <url href="https://en.wikipedia.org/wiki/Koch%27s_postulates" visual="wikipedia.org"><em>Koch's postulates</em></url> which have historically been used to determine whether a particular organism causes a disease. The basic idea is that the organism should be present in people with the disease, and not present in those without it <mdash/> thus, a treatment that eliminates the organism should also eliminate the disease. Further, infecting someone with the organism should cause them to contract the disease. An example of this was seen in the work of Dr. Barry Marshall, who had a hypothesis that stomach ulcers were caused by a bacterium (<em>Helicobacter pylori</em>). To demonstrate this, he infected himself with the bacterium, and soon thereafter developed severe inflammation in his stomach. He then treated himself with an antibiotic, and his stomach soon recovered. He later won the Nobel Prize in Medicine for this work.
    </p>
    <p>
      Often we would like to test causal hypotheses but we can't actually do an experiment, either because it's impossible (<q>What is the relationship between human carbon emissions and the earth's climate?</q>) or unethical (<q>What are the effects of severe abuse on child brain development?</q>). However, we can still collect data that might be relevant to those questions. For example, we can potentially collect data from children who have been abused as well as those who have not, and we can then ask whether their brain development differs.
    </p>
    <p>
      Let's say that we did such an analysis, and we found that abused children had poorer brain development than non-abused children. Would this demonstrate that abuse <em>causes</em> poorer brain development? No. Whenever we observe a statistical association between two variables, it is certainly possible that one of those two variables causes the other. However, it is also possible that both of the variables are being influenced by a third variable; in this example, it could be that child abuse is associated with family stress, which could also cause poorer brain development through less intellectual engagement, food stress, or many other possible avenues. The point is that a correlation between two variables generally tells us that something is <em>probably</em> causing something else, but it doesn't tell us what is causing what.
    </p>

    <subsection xml:id="subsec-causal-graphs">
      <title>Causal graphs</title>
      <p>
        One useful way to describe causal relations between variables is through a <em>causal graph</em>, which shows variables as circles and causal relations between them as arrows. For example, <xref ref="fig-simple-causal-graph"/> shows the causal relationships between study time and two variables that we think should be affected by it: exam grades and exam finishing times.
      </p>
      <p>
        However, in reality the effects on finishing time and grades are not due directly to the amount of time spent studying, but rather to the amount of knowledge that the student gains by studying. We would usually say that knowledge is a <em>latent</em> variable <mdash/> that is, we can't measure it directly but we can see it reflected in variables that we can measure (like grades and finishing times). <xref ref="fig-latent-causal-graph"/> shows this.
      </p>

      <figure xml:id="fig-simple-causal-graph">
        <caption>A graph showing causal relationships between three variables: study time, exam grades, and exam finishing time. A green arrow represents a positive relationship (i.e. more study time causes exam grades to increase), and a red arrow represents a negative relationship (i.e. more study time causes faster completion of the exam).</caption>
        <image source="dag_example.png" width="50%">
          <description>A directed acyclic graph showing three circular nodes: Study Time at the top, with green arrow pointing to Exam Grade (positive effect) and red arrow pointing to Finishing Time (negative effect).</description>
        </image>
      </figure>

      <figure xml:id="fig-latent-causal-graph">
        <caption>A graph showing the same causal relationships as above, but now also showing the latent variable (knowledge) using a square box.</caption>
        <image source="dag_latent_example.png" width="50%">
          <description>A directed acyclic graph similar to the previous one, but with an additional square node labeled Knowledge mediating between Study Time and the outcomes (Exam Grade and Finishing Time).</description>
        </image>
      </figure>

      <p>
        Here we would say that knowledge <em>mediates</em> the relationship between study time and grades/finishing times. That means that if we were able to hold knowledge constant (for example, by administering a drug that causes immediate forgetting), then the amount of study time should no longer have an effect on grades and finishing times.
      </p>
      <p>
        Note that if we simply measured exam grades and finishing times we would generally see negative relationship between them, because people who finish exams the fastest in general get the highest grades. However, if we were to interpret this correlation as a causal relation, this would tell us that in order to get better grades, we should actually finish the exam more quickly! This example shows how tricky the inference of causality from non-experimental data can be.
      </p>
      <p>
        Within statistics and machine learning, there is a very active research community that is currently studying the question of when and how we can infer causal relationships from non-experimental data. However, these methods often require strong assumptions, and must generally be used with great caution.
      </p>
    </subsection>
  </section>

  <section xml:id="sec-learning-objectives-cont-rel">
    <title>Learning objectives</title>
    <p>
      After reading this chapter, you should be able to:
    </p>
    <ul>
      <li><p>Describe the concept of the correlation coefficient and its interpretation</p></li>
      <li><p>Compute the correlation between two continuous variables</p></li>
      <li><p>Describe the effect of outlier data points and how to address them</p></li>
      <li><p>Describe the potential causal influences that can give rise to an observed correlation</p></li>
    </ul>
  </section>

  <section xml:id="sec-suggested-readings-cont-rel">
    <title>Suggested readings</title>
    <ul>
      <li><p><url href="http://bayes.cs.ucla.edu/WHY/" visual="bayes.cs.ucla.edu"><em>The Book of Why</em></url> by Judea Pearl <mdash/> an excellent introduction to the ideas behind causal inference.</p></li>
    </ul>
  </section>

  <section xml:id="sec-appendix-cont-rel">
    <title>Appendix</title>

    <subsection xml:id="subsec-gini-index">
      <title>Quantifying inequality: The Gini index</title>
      <p>
        Before we look at the analysis reported in the story, it's first useful to understand how the Gini index is used to quantify inequality. The Gini index is usually defined in terms of a curve that describes the relation between income and the proportion of the population that has income at or less than that level, known as a <em>Lorenz curve</em>. However, another way to think of it is more intuitive: It is the relative mean absolute difference between incomes, divided by two (from <url href="https://en.wikipedia.org/wiki/Gini_coefficient" visual="wikipedia.org">Wikipedia</url>):
      </p>
      <me>
        G = \frac{\displaystyle{\sum_{i=1}^n \sum_{j=1}^n \left| x_i - x_j \right|}}{\displaystyle{2n\sum_{i=1}^n x_i}}
      </me>

      <figure xml:id="fig-gini-lorenz">
        <caption>Lorenz curves for A) perfect equality, B) normally distributed income, and C) high inequality (equal income except for one very wealthy individual).</caption>
        <image source="gini0.png" width="80%">
          <description>Three panels showing Lorenz curves. Panel A shows a straight diagonal line (Gini=0) representing perfect equality. Panel B shows a curved line (Gini=0.09) for normally distributed income. Panel C shows a highly curved line (Gini=0.90) representing extreme inequality with one wealthy outlier.</description>
        </image>
      </figure>

      <p>
        <xref ref="fig-gini-lorenz"/> shows the Lorenz curves for several different income distributions. The top left panel (A) shows an example with 10 people where everyone has exactly the same income. The length of the intervals between points are equal, indicating each person earns an identical share of the total income in the population. The top right panel (B) shows an example where income is normally distributed. The bottom left panel shows an example with high inequality; everyone has equal income ($40,000) except for one person, who has income of $40,000,000. According to the US Census, the United States had a Gini index of 0.469 in 2010, falling roughly half way between our normally distributed and maximally inequal examples.
      </p>
    </subsection>

    <subsection xml:id="subsec-bayesian-correlation">
      <title>Bayesian correlation analysis</title>
      <p>
        We can also analyze the FiveThirtyEight data using Bayesian analysis, which has two advantages. First, it provides us with a posterior probability <mdash/> in this case, the probability that the correlation value exceeds zero. Second, the Bayesian estimate combines the observed evidence with a <em>prior</em>, which has the effect of <em>regularizing</em> the correlation estimate, effectively pulling it towards zero. Here we can compute it using <em>BayesFactor</em> package in R.
      </p>
      <p>
        The Bayesian analysis gives a Bayes Factor of 20.46796 in favor of the alternative hypothesis (correlation not equal to zero). The posterior distribution has:
      </p>
      <ul>
        <li><p>Median correlation: 0.38</p></li>
        <li><p>89% Credible Interval: [0.16, 0.58]</p></li>
      </ul>
      <p>
        Notice that the correlation estimated using the Bayesian method (0.38) is slightly smaller than the one estimated using the standard correlation coefficient (0.42), which is due to the fact that the estimate is based on a combination of the evidence and the prior, which effectively shrinks the estimate toward zero. However, notice that the Bayesian analysis is not robust to the outlier, and it still says that there is fairly strong evidence that the correlation is greater than zero (with a Bayes factor of more than 20).
      </p>
    </subsection>
  </section>

</chapter>
